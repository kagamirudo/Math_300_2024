{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 300: Recitation 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start today by doing an example of the most asked about question of the quarter so far. We will do an example of calculating the error bound of a Lagrange Polynomial and will show how this changes when we do the same process for a Hermite Polynomial. Gonna do this on the board (feel free to take notes on it or take a picture of the process!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1.1a/4.1.3a\n",
    "\n",
    "Use the forward and backward difference formulas to determine each missing entry in the following table:\n",
    "\n",
    "| $x$ | $f(x)$ | $f'(x)$ |\n",
    "| --- | --- | --- |\n",
    "| $0.5$ | $0.4794$ | |\n",
    "| $0.6$ | $0.5646$ | |\n",
    "| $0.7$ | $0.6442$ | |\n",
    "\n",
    "This data was generated with $f(x) = \\sin(x)$. Compute the actual errors, and find the error bounds with the error formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n",
    "\n",
    "For the first entry, i.e., the approximation of $f'(0.5)$, we are forced to use the forward difference formula (as we have no points to the left of $x = 0.5$ to use the backward difference formula). We obtain $\\newline \\newline$\n",
    "$$\n",
    "f'(0.5) \\approx \\frac{0.5646 - 0.4794}{0.6 - 0.5} = 0.852.\n",
    "\\newline\n",
    "$$\n",
    "Likewise, for the entry approximating $f'(0.7)$, we are forced to use the backward difference approximation. We find $\\newline \\newline$\n",
    "$$\n",
    "f'(0.7) \\approx \\frac{0.6442 - 0.5646}{0.7 - 0.6} = 0.796.\n",
    "\\newline\n",
    "$$\n",
    "For the middle entry, we have flexibility. We could either do the forward difference, corresponding to using the information from $x = 0.7$, or we could do the backward difference corresponding to the information from $x = 0.5$. The two valid tables are therefore\n",
    "\n",
    "| $x$ | $f(x)$ | $f'(x)$ |\n",
    "| --- | --- | --- |\n",
    "| $0.5$ | $0.4794$ | $0.852$ |\n",
    "| $0.6$ | $0.5646$ | $0.852$ |\n",
    "| $0.7$ | $0.6442$ | $0.796$ |\n",
    "\n",
    "and\n",
    "\n",
    "| $x$ | $f(x)$ | $f'(x)$ |\n",
    "| --- | --- | --- |\n",
    "| $0.5$ | $0.4794$ | $0.852$ |\n",
    "| $0.6$ | $0.5646$ | $0.796$ |\n",
    "| $0.7$ | $0.6442$ | $0.796$ |\n",
    "\n",
    "We can compute the actual error using the fact that $f'(x) = \\cos(x)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02558256189037278"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error at x = 0.5\n",
    "abs(cos(0.5) - 0.852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02666438509032165"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backward difference error at x = 0.6\n",
    "abs(cos(0.6) - 0.852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029335614909678287"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward difference error at x = 0.6\n",
    "abs(cos(0.6) - 0.796)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031157812715511546"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# error at x = 0.7\n",
    "abs(cos(0.7) - 0.796)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the forward and backward difference at $x = 0.6$ are the same distance on either side of the true solution. This means their average should be even more accurate. This will motivate the centered difference scheme later on. For now, we can use the error bound obtained directly from the Taylor expansion to see that the errors we obtained do indeed sit below the theoretical bound: For $x = 0.5$ we have $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\Big|f'(0.5) - \\frac{f(0.6) - f(0.5)}{0.6 - 0.5}\\Big| &\\leq \\frac{0.6 - 0.5}{2}\\max_{\\xi \\in [0.5, 0.6]}|f''(\\xi)| \\\\\n",
    "    &\\leq 0.05\\sin(0.6) \\approx 0.028232,\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "which contains our error of $0.02558$.\n",
    "\n",
    "For $x = 0.7$, we have $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\Big|f'(0.7) - \\frac{f(0.7) - f(0.6)}{0.7 - 0.6}\\Big| &\\leq \\frac{0.7 - 0.6}{2}\\max_{\\xi \\in [0.6, 0.7]}|f''(\\xi)| \\\\\n",
    "    &\\leq 0.05\\sin(0.7) \\approx 0.03221,\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "which contains our error of $0.03116$.\n",
    "\n",
    "For $x = 0.6$, we have for the forward difference $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\Big|f'(0.6) - \\frac{f(0.7) - f(0.6)}{0.7 - 0.6}\\Big| &\\leq \\frac{0.7 - 0.6}{2}\\max_{\\xi \\in [0.6, 0.7]}|f''(\\xi)| \\\\\n",
    "    &\\leq 0.05\\sin(0.7) \\approx 0.03221,\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "which contains our error of $0.02934$.\n",
    "\n",
    "For the backward difference, we have $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\Big|f'(0.6) - \\frac{f(0.6) - f(0.5)}{0.6 - 0.5}\\Big| &\\leq \\frac{0.6 - 0.5}{2}\\max_{\\xi \\in [0.5, 0.6]}|f''(\\xi)| \\\\\n",
    "    &\\leq 0.05\\sin(0.6) \\approx 0.028232,\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "which contains our error of $0.02666$.\n",
    "\n",
    "A note beyond the scope of the class: Although it seems arbitrary whether we select forward or backward difference schemes for data in the \"middle\" of our dataset, there are places where it makes a big difference! In numerical differential equations (which will likely be covered in the second quarter of the class), it can be found that backward Euler based schemes for discretizing your differential equations are (unconditionally) stable (meaning they don't experience any numerical blow-up or anything horrible that would corrupt your solution), whereas forward Euler based methods do not share this luxury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.2.1b\n",
    "\n",
    "Apply the extrapolation process described in Example 1 to determine $N_{3}(h)$, an approximation to $f'(x_{0})$, for the following function and step size. $\\newline \\newline$\n",
    "$$\n",
    "f(x) = x + e^{x}, \\;\\; x_{0} = 0.0, \\;\\; h = 0.4.\n",
    "\\newline\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "We expect to get something close to $2$, since $\\newline \\newline$\n",
    "$$\n",
    "f'(x_{0}) = 1 + e^{0} = 2.\n",
    "\\newline\n",
    "$$\n",
    "Equation (4.16) in section 4.2 of the textbook tells us that a recursive formula for computing $N_{j}(h)$ is $\\newline \\newline$\n",
    "$$\n",
    "N_{j}(h) = N_{j - 1}\\Big(\\frac{h}{2}\\Big) + \\frac{N_{j - 1}(h/2) - N_{j - 1}(h)}{4^{j - 1} - 1}\n",
    "\\newline\n",
    "$$\n",
    "for each $j = 2, 3, \\dots$, and for $j = 1$, we just use our \"usual\" methods to make a low-order approximation (this particular formula is valid for quadratic schemes). To compute $N_{3}(h)$, we begin by simply computing $N_{1}(h)$ (here we use centered differences to obtain that quadratic order): $\\newline \\newline$\n",
    "$$\n",
    "f'(0.0) \\approx N_{1}(0.4) = \\frac{f(0.4) - f(-0.4)}{0.8} \\approx 2.0269.\n",
    "\\newline\n",
    "$$\n",
    "This is already pretty close to our expected result (notice that it is about $\\mathcal{O}(h^{2}) \\asymp \\mathcal{O}(10^{-2})$ away from our desired solution), but we can do better. To compute $N_{2}(h)$, we first need $N_{1}(h/2)$, which is $\\newline \\newline$\n",
    "$$\n",
    "f'(0.0) \\approx N_{1}(0.2) = \\frac{f(0.2) - f(-0.2)}{0.4} \\approx 2.0067.\n",
    "\\newline\n",
    "$$\n",
    "Then, $\\newline \\newline$\n",
    "$$\n",
    "N_{2}(0.4) = N_{1}(0.2) + \\frac{N_{1}(0.2) - N_{1}(0.4)}{3} \\approx 1.999966.\n",
    "\\newline\n",
    "$$\n",
    "We can see how well this approximation is doing already, but let's continue. We need $N_{2}(h/2)$ to compute $N_{3}(h)$, and to compute $N_{2}(h/2)$, we need $N_{1}(h/4)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{1}(0.1) = \\frac{f(0.1) - f(-0.1)}{0.2} \\approx 2.0017.\n",
    "\\newline\n",
    "$$\n",
    "Now we can compute $N_{2}(h/2) = N_{2}(0.2)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{2}(0.2) = N_{1}(0.1) + \\frac{N_{1}(0.1) - N_{1}(0.2)}{3} \\approx 2.000033.\n",
    "$$\n",
    "Finally, we have what we need to compute $N_{3}(h) = N_{3}(0.4)$: $\\newline \\newline$\n",
    "$$\n",
    "N_{3}(0.4) = N_{2}(0.2) + \\frac{N_{2}(0.2) - N_{2}(0.4)}{15} \\approx 2.000000.\n",
    "\\newline\n",
    "$$\n",
    "We've obtained a very accurate solution at only a small cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.3.1a\n",
    "\n",
    "Approximate the following integral with the trapezoidal rule. $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx\n",
    "\\newline\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "The book lists the \"trapezoidal rule\" as being $\\newline \\newline$\n",
    "$$\n",
    "\\int_{a}^{b}f(x)dx \\approx \\frac{b - a}{2}(f(a) + f(b)),\n",
    "\\newline\n",
    "$$\n",
    "but we can do better. Indeed, instead of essentially using one interpolant over the whole interval, we can partition the interval, interpolate over each subinterval, and sum the resulting integrals. This produces the following \"generalization\" (which is what most people actually mean when they refer to this scheme in practice): $\\newline \\newline$\n",
    "$$\n",
    "\\int_{a}^{b}f(x)dx \\approx \\frac{h}{2}\\Big(f(a) + 2f(a + h) + 2f(a + 2h) + \\dots + 2f(a + Nh) + f(b)\\Big).\n",
    "\\newline\n",
    "$$\n",
    "Here, we assume uniform spacing, and that there are $N$ interior nodes $a + nh$, for $n \\in \\{1, \\dots, N\\}$.\n",
    "\n",
    "So, instead of doing the boring thing (which would be to make the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx \\approx \\frac{1 - 0.5}{2}(0.5^{4} + 1^{4}) = 0.25 \\cdot (1.0625) = 0.265625\n",
    "\\newline\n",
    "$$\n",
    "for our answer), we'll split up the interval with $h = 0.1$ to obtain $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx = \\int_{0.5}^{0.6}x^{4}dx + \\int_{0.6}^{0.7}x^{4}dx + \\int_{0.7}^{0.8}x^{4}dx + \\int_{0.8}^{0.9}x^{4}dx + \\int_{0.9}^{1}x^{4}dx.\n",
    "\\newline\n",
    "$$\n",
    "Now, we make the approximation $\\newline \\newline$\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\int_{0.5}^{1}x^{4}dx &\\approx \\frac{0.1}{2}(f(0.5) + 2f(0.6) + 2f(0.7) + 2f(0.8) + 2f(0.9) + f(1)) \\\\\n",
    "    &\\approx 0.05(0.0625 + 0.2592 + 0.4802 + 0.8192 + 1.3122 + 1) \\\\\n",
    "    &\\approx 0.1967.\n",
    "\\end{align*}\n",
    "\\newline\n",
    "$$\n",
    "Of course, we can integrate by hand to see which is the better approximation: $\\newline \\newline$\n",
    "$$\n",
    "\\int_{0.5}^{1}x^{4}dx = \\frac{x^{5}}{5}\\Big|_{0.5}^{1} = 0.2 - \\frac{0.5^{5}}{5} = 0.2 - 0.1 \\cdot 0.5^{4} = 0.2 - 0.1 \\cdot 0.0625 = 0.2 - 0.00625 = 0.19375.\n",
    "\\newline\n",
    "$$\n",
    "As expected, the approximation with the finer mesh is closer to the true solution than with the \"one-step\" mesh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.1.29\n",
    "\n",
    "Consider the function $\\newline \\newline$\n",
    "$$\n",
    "e(h) = \\frac{\\epsilon}{h} + \\frac{h^{2}}{6}M,\n",
    "\\newline\n",
    "$$\n",
    "where $M$ is a bound on the third derivative of a function. Show that $e(h)$ has a minimum at $\\sqrt[3]{3\\epsilon/M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: \n",
    "\n",
    "All we need to do is perform the standard optimization techniques from calculus. The derivative is $\\newline \\newline$\n",
    "$$\n",
    "e'(h) = -\\frac{\\epsilon}{h^{2}} + \\frac{h}{3}M,\n",
    "\\newline\n",
    "$$\n",
    "which means that a critical point $h$ must satisfy $\\newline \\newline$\n",
    "$$\n",
    "\\frac{\\epsilon}{h^{2}} = \\frac{h}{3}M.\n",
    "\\newline\n",
    "$$\n",
    "Solving for $h$, we find that $h = \\sqrt[3]{3\\epsilon/M}$ is a critical point. Further, it is indeed a minimum value since $\\newline \\newline$\n",
    "$$\n",
    "e''(h) = \\frac{2\\epsilon}{h^{3}} + \\frac{M}{3} > 0\n",
    "\\newline\n",
    "$$\n",
    "when $\\epsilon$, $h$ and $M$ are positive (of course, this is a reasonable assumption to make).\n",
    "\n",
    "The reason for picking this exercise is obviously not for its mathematical content; rather, what this exercise shows us is that if we have a formula with an error term for a numerical scheme, we can optimize it in this way to find an optimal selection for our step size $h$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
